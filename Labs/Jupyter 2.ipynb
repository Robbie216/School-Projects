{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ApxlRHkvsnTo"
   },
   "source": [
    "# Working with Twitter data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2GH8LhVDsnTq"
   },
   "source": [
    "In this lecture, you will learn how to preprocess actual twitter data and create word embeddings from a pre-trained source. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "88ax7IYQsnTr"
   },
   "source": [
    "## Import the necessary modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "P-XuZ8jysnTs"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "np.random.seed(0)\n",
    "from keras.models import Model\n",
    "from keras.layers import Dense, Input, Dropout, LSTM, Activation, SimpleRNN, GRU\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.preprocessing import sequence\n",
    "from keras.initializers import glorot_uniform"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "M6sIZLyZsnTt"
   },
   "source": [
    "## Do some initial cleaning using the twitter preprocessing Python package\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 231
    },
    "id": "utuI-yE3snTv",
    "outputId": "0a17c48e-7ba0-4f40-b667-39d535102954"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ekide\\Documents\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "442"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#step 1: bring in the cleaned up twitter data: used preprocessing python package to clean up the initial twitter data\n",
    "%cd C:\\Users\\ekide\\Documents\n",
    "import pandas as pd\n",
    "twitter_labeled = pd.read_csv(r\"C:\\Users\\ekide\\Downloads\\US_AMEX_Tweets_recent2_MV_labels2_05012020_05042020_v2.csv\")#need to save as utf-8 .csv file from excel or other source\n",
    "twitter_labeled2=twitter_labeled.drop_duplicates()\n",
    "dups=pd.DataFrame(twitter_labeled2['text'].value_counts()).rename(columns={'text':'count'})\n",
    "twitter_labeled3=pd.merge(twitter_labeled2, dups, left_on='text', right_index=True, how=\"inner\")\n",
    "twitter_labeled4=twitter_labeled3.drop(axis=1, index=[226, 760, 235, 757, 247, 759, 535, 330, 680, 780])\n",
    "twitter_labeled4.drop('count', axis=1, inplace=True)\n",
    "len(twitter_labeled4) #we have 442 unique tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 163
    },
    "id": "Oyadp0YZsnTw",
    "outputId": "69c4222f-2329-4883-d37d-fe7b7c252241"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['text', 'Label'], dtype='object')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "twitter_labeled4.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rO0yIrwtsnTw"
   },
   "source": [
    "## Since the goal here is to capture negative sentiment, let us convert into 2 classes only: 'negative' and 'non-negative'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 231
    },
    "id": "YQOnYBgEsnTx",
    "outputId": "c2c199f8-f0f5-4b13-ef19-342915f5104f"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(300, 142, 300, 142, 442, 442)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#step 2: only consider 2 classes, negative and non negative, since we want to capture the negatuve sentiment: create\n",
    "#training and test sets\n",
    "twitter_labeled4.loc[:, 'Label_comb']=twitter_labeled4['Label'].map(lambda x: 0 if x==-1 else 1)\n",
    "twitter_labeled4['Label_comb'].value_counts()\n",
    "neg_tweets=twitter_labeled4[twitter_labeled4['Label_comb']==0]\n",
    "non_neg_tweets=twitter_labeled4[twitter_labeled4['Label_comb']==1]\n",
    "X_train=np.array([x for x in non_neg_tweets['text'][:150]]+[x for x in neg_tweets['text'][:150]])\n",
    "Y_train=np.array([x for x in non_neg_tweets['Label_comb'][:150]]+[x for x in neg_tweets['Label_comb'][:150]])\n",
    "X_test=np.array([x for x in non_neg_tweets['text'][150:]]+[x for x in neg_tweets['text'][150:]])\n",
    "Y_test=np.array([x for x in non_neg_tweets['Label_comb'][150:]]+[x for x in neg_tweets['Label_comb'][150:]])\n",
    "X_train_test=np.array([x for x in X_train]+[x for x in X_test])\n",
    "y_train_test=np.array([x for x in Y_train]+[x for x in Y_test])\n",
    "len(X_train), len(X_test), len(Y_train), len(Y_test), len(X_train_test), len(y_train_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wPqZTJ9_snTx"
   },
   "source": [
    "## Import pre-trained glove word embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 350
    },
    "id": "CSNrRQKXsnTy",
    "outputId": "34a9fe23-3935-4c5f-a497-5a23c9cf4ba3"
   },
   "outputs": [],
   "source": [
    "#step 3: bring in word embedding pre-trained vectors: \n",
    "import numpy as np\n",
    "glove_twitter_file = 'glove.twitter.27B.50d.txt'\n",
    "def read_glove_vecs(glove_file):\n",
    "    with open(glove_file, 'r', encoding='utf-8-sig') as f:\n",
    "        words = set()\n",
    "        word_to_vec_map = {}\n",
    "        for line in f:\n",
    "            line = line.strip().split()#removes leading and trailing spaces\n",
    "            curr_word = line[0]\n",
    "            words.add(curr_word)\n",
    "            word_to_vec_map[curr_word] = np.array(line[1:], dtype=np.float64)\n",
    "        \n",
    "        i = 1\n",
    "        words_to_index = {}\n",
    "        index_to_words = {}\n",
    "        for w in sorted(words):\n",
    "            words_to_index[w] = i\n",
    "            index_to_words[i] = w\n",
    "            i = i + 1\n",
    "    return words_to_index, index_to_words, word_to_vec_map\n",
    "words_to_index, index_to_words, word_to_vec_map=read_glove_vecs(glove_twitter_file)\n",
    "wordsintwitterglove=[]\n",
    "for value in index_to_words.values(): \n",
    "    wordsintwitterglove.append(value)\n",
    "#almost 2MM words\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "Er--JOaSsnTy"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1193514"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(wordsintwitterglove)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "FwpR9JWBsnTz"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The word 'cards' in the embedding space is: \n",
      "[ 0.49513   0.0292   -0.41095   0.2833    0.94241  -0.37731   0.84692\n",
      " -0.51128   0.45714  -0.58924   0.94307   0.90303  -3.4979    0.29424\n",
      " -0.26355   0.42858  -0.38724  -0.47719   0.044124  0.49529  -1.1181\n",
      " -0.48781   0.60082  -0.64361  -0.71212   0.36798  -0.059819 -0.58809\n",
      "  0.67646  -0.75717   0.4728   -0.23525  -0.29401  -0.13993   0.69861\n",
      " -0.29542  -0.013882  0.065944 -0.38697   0.23558   0.50186   0.09126\n",
      "  0.4026   -0.39129   0.73219  -0.52371  -0.048465 -1.2898   -0.022145\n",
      "  0.42831 ]\n"
     ]
    }
   ],
   "source": [
    "print(\"The word 'cards' in the embedding space is: \")\n",
    "print(word_to_vec_map['cards'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zsLnDIKvsnT0"
   },
   "source": [
    "## Typical steps to clean the data for sentiment analysis: \n",
    "* Remove hash tags and other distracting symbols\n",
    "* Remove numbers\n",
    "* Convert to lower case\n",
    "* Remove unnecessary punctuation\n",
    "* Remove stop words: unnecessary/generic words\n",
    "* Stem sentences: only retain the main roots\n",
    "* Tokenize sentences: create a list of words from the final cleaned string above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting num2words\n",
      "  Downloading num2words-0.5.10-py3-none-any.whl (101 kB)\n",
      "Collecting docopt>=0.6.2\n",
      "  Downloading docopt-0.6.2.tar.gz (25 kB)\n",
      "Building wheels for collected packages: docopt\n",
      "  Building wheel for docopt (setup.py): started\n",
      "  Building wheel for docopt (setup.py): finished with status 'done'\n",
      "  Created wheel for docopt: filename=docopt-0.6.2-py2.py3-none-any.whl size=12879 sha256=f7a032c0c1067e23b1e7d3b29df55dbe4140318e216d3d6d43bf34844946fe9b\n",
      "  Stored in directory: c:\\users\\ekide\\appdata\\local\\pip\\cache\\wheels\\3f\\2a\\fa\\4d7a888e69774d5e6e855d190a8a51b357d77cc05eb1c097c9\n",
      "Successfully built docopt\n",
      "Installing collected packages: docopt, num2words\n",
      "Successfully installed docopt-0.6.2 num2words-0.5.10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: You are using pip version 21.2.4; however, version 21.3 is available.\n",
      "You should consider upgrading via the 'd:\\anaconda\\envs\\dl4denv\\python.exe -m pip install --upgrade pip' command.\n"
     ]
    }
   ],
   "source": [
    "!pip install num2words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "lrTUbh85snT0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original sentence is: \n",
      " \n",
      "I used to work for American Express and spent some time in NY. It was on her bucket list to visit, so I took her there a year and a half ago. We love living around no one, but NYC is a fun place to visit! Well probably not so much at the moment, but we’ll get through this!\n",
      " \n",
      "Tokenized sentence is now: \n",
      " \n",
      "['i', 'used', 'to', 'work', 'for', 'american', 'express', 'and', 'spent', 'some', 'time', 'in', 'ny', 'it', 'was', 'on', 'her', 'bucket', 'list', 'to', 'visit', 'so', 'i', 'took', 'her', 'there', 'a', 'year', 'and', 'a', 'half', 'ago', 'we', 'love', 'living', 'around', 'no', 'one', 'but', 'nyc', 'is', 'a', 'fun', 'place', 'to', 'visit', 'well', 'probably', 'not', 'so', 'much', 'at', 'the', 'moment', 'but', 'well', 'get', 'through', 'this']\n"
     ]
    }
   ],
   "source": [
    "#step 4: function to clean this twitter data some more to create words that can be mapped to the embedding matrix: \n",
    "#function to take in a sentence and return a list of words, which are more ready to be vectorized by the embedding matrix: \n",
    "def clean_sentence(sentence): \n",
    "    import re\n",
    "    import num2word\n",
    "    from num2words import num2words\n",
    "    punc_to_remove='’!\"#%&\\'()*+,/:;<=>?@[\\\\]^_`{|}~' #exclude dollar sign and dot as we want to first convert dollar amounts\n",
    "    #and decimals to word numerics, and then we can remove these punctuation signs: \n",
    "    words=re.split('\\s+',sentence.lower())\n",
    "    table = str.maketrans('', '', punc_to_remove) #remove punctuation as it could be at the end of the word\n",
    "    words = [w.translate(table) for w in words]\n",
    "    words=[x for x in words if x!='']\n",
    "    sentence=\" \".join(words)\n",
    "    #first replace only the actual numbers with words: \n",
    "    th_pattern='\\d+th'\n",
    "    th_numbers_to_fix=re.findall(th_pattern, sentence) #list of th numbers to convert to numerics\n",
    "    dollar_pattern='\\$\\.?0?\\d+\\.?\\d*'\n",
    "    dollars_to_fix=re.findall(dollar_pattern, sentence)\n",
    "    words=re.split('\\s+',sentence.lower())\n",
    "    remove_numerics=[]\n",
    "    for word in words: \n",
    "        try:\n",
    "            if(word in th_numbers_to_fix):            \n",
    "                remove_numerics.append(num2words(int(float(word.replace('th',''))), to='ordinal'))\n",
    "            elif (word in dollars_to_fix): \n",
    "                number=int(float(word.replace('$','')))\n",
    "                if(number==1): \n",
    "                    remove_numerics.append(num2words(int(float(word.replace('$','')))))\n",
    "                    remove_numerics.append('dollar')\n",
    "                else: \n",
    "                    remove_numerics.append(num2words(int(float(word.replace('$','')))))\n",
    "                    remove_numerics.append('dollars')  \n",
    "            else:\n",
    "                number=int(float(word)) #convert decimal or integer string into an integer\n",
    "                remove_numerics.append(num2word.word(number))\n",
    "        except: \n",
    "            remove_numerics.append(word)\n",
    "    punc_to_remove='$.' #exclude dollar sign\n",
    "    sentence=\" \".join(remove_numerics)\n",
    "    words=re.split('\\s+',sentence.lower())\n",
    "    table = str.maketrans('', '', punc_to_remove) #remove punctuation as it could be at the end of the word\n",
    "    words = [w.translate(table) for w in words]\n",
    "    words=[x for x in words if x not in ('', '-')]\n",
    "    return words\n",
    "sentence=\"I used to work for American Express and spent some time in NY. It was on her bucket list to visit, so I took her there a year and a half ago. We love living around no one, but NYC is a fun place to visit! Well probably not so much at the moment, but we’ll get through this!\"\n",
    "words=clean_sentence(sentence)\n",
    "print(\"Original sentence is: \")\n",
    "print(\" \")\n",
    "print(sentence)\n",
    "print(\" \")\n",
    "print(\"Tokenized sentence is now: \")\n",
    "print(\" \")\n",
    "print(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "hhPMEkyAsnT3"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\ekide\\\\Documents'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oZbmet95snT4"
   },
   "source": [
    "## Now, let us look at our training and test sets: \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "5NGYZS2rsnT7"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Question of the Day - Is it worth the effort to get the Hilton Ascend credit card that can be linked to Wyndham Diamond and Caesars Diamond status rewards cards?',\n",
       "       'Today’s Digital Transactions News: T&E Plunge Hammers AmEx; PayFac Volume To Hit $4 Trillion by 2025; Plus Weekly Recap'],\n",
       "      dtype='<U280')"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train[0:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "ExZgIagasnT8"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['rumor has it amex might be planning additional benefits to their platinum card... meanwhile the sapphire took a downfall with their recent updates (imo anyway....the benefits are useless to me making the price increase not worth it)',\n",
       "       '[Targeted] AmEx Offer: , Spend $100+ & Receive $30 Statement Credit + $50 Off $150+'],\n",
       "      dtype='<U280')"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test[0:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GMA0urrjsnT8"
   },
   "source": [
    "### Step 1: find the maximum number of words across all sentences. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "9aEPFvf1snT_"
   },
   "outputs": [],
   "source": [
    "def sentences_to_indices(X, word_to_index, max_len):\n",
    "    \"\"\"\n",
    "    Converts an array of sentences (strings) into an array of indices corresponding to words in the sentences.\n",
    "    The output shape should be such that it can be given to `Embedding()` \n",
    "    \n",
    "    Arguments:\n",
    "    X -- array of sentences (strings), of shape (m, 1)\n",
    "    word_to_index -- a dictionary containing the each word mapped to its index\n",
    "    max_len -- maximum number of words in a sentence. You can assume every sentence in X is no longer than this. \n",
    "    \n",
    "    Returns:\n",
    "    X_indices -- array of indices corresponding to words in the sentences from X, of shape (m, max_len)\n",
    "    \"\"\"\n",
    "    \n",
    "    m = X.shape[0]                                   # number of training examples\n",
    "    \n",
    "    # Initialize X_indices as a numpy matrix of zeros and the correct shape (≈ 1 line)\n",
    "    X_indices = np.zeros((m,max_len))\n",
    "    \n",
    "    for i in range(m):                               # loop over training examples\n",
    "        \n",
    "        # Convert the ith training sentence in lower case and split is into words. You should get a list of words.\n",
    "        sentence_words=clean_sentence(X[i])\n",
    "        # Initialize j to 0\n",
    "        j = 0\n",
    "        \n",
    "        # Loop over the words of sentence_words\n",
    "        for w in sentence_words:\n",
    "            # Set the (i,j)th entry of X_indices to the index of the correct word.\n",
    "            if (w in wordsintwitterglove): \n",
    "                X_indices[i, j] = word_to_index[w]\n",
    "                # Increment j to j + 1\n",
    "                j = j+1\n",
    "            \n",
    "\n",
    "    \n",
    "    return X_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "iPtRPkEdsnUB"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X1 = ['Question of the Day - Is it worth the effort to get the Hilton Ascend credit card that can be linked to Wyndham Diamond and Caesars Diamond status rewards cards?'\n",
      " 'Today’s Digital Transactions News: T&E Plunge Hammers AmEx; PayFac Volume To Hit $4 Trillion by 2025; Plus Weekly Recap']\n",
      "X1_indices = [[503357. 446383. 601627. 138215. 283380. 284816. 657072. 601627. 176556.\n",
      "  607687. 227866. 601627. 258111.  37963. 127558.  96730. 601405.  94350.\n",
      "   59105. 342820. 607687. 658588. 151100.  26338.  91709. 151100. 573728.\n",
      "  519712.  96819.      0.      0.      0.      0.      0.      0.      0.\n",
      "       0.      0.      0.      0.      0.      0.      0.      0.      0.\n",
      "       0.      0.      0.      0.      0.      0.      0.      0.      0.\n",
      "       0.      0.      0.      0.      0.]\n",
      " [608104. 153374. 612987. 406728. 593762. 483492. 248627.  24377. 644128.\n",
      "  607687. 259017. 213327. 163389. 615083.  89093. 621963. 603283. 620819.\n",
      "  208201. 483525. 650553. 510109.      0.      0.      0.      0.      0.\n",
      "       0.      0.      0.      0.      0.      0.      0.      0.      0.\n",
      "       0.      0.      0.      0.      0.      0.      0.      0.      0.\n",
      "       0.      0.      0.      0.      0.      0.      0.      0.      0.\n",
      "       0.      0.      0.      0.      0.]]\n"
     ]
    }
   ],
   "source": [
    "#testing the above: \n",
    "X1 =  X_train[0:2]\n",
    "maxlen=max(max([len(x.split()) for x in X_train]), max([len(x.split()) for x in X_test]))\n",
    "X1_indices = sentences_to_indices(X1,words_to_index, max_len = maxlen)\n",
    "print(\"X1 =\", X1)\n",
    "print(\"X1_indices =\", X1_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BqZplzLdsnUG"
   },
   "outputs": [],
   "source": [
    "#notice above that the words need to be padded to make sure each input sentence takes up the same amount of static\n",
    "#memory when being fed into a Recurrent Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "za2wn5kSsnUI"
   },
   "source": [
    "### Step 2: Define an embedding layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "0ZgoCQtJsnUI"
   },
   "outputs": [],
   "source": [
    "def pretrained_embedding_layer(word_to_vec_map, word_to_index):\n",
    "    \"\"\"\n",
    "    Creates a Keras Embedding() layer and loads in pre-trained GloVe 50-dimensional vectors.\n",
    "    \n",
    "    Arguments:\n",
    "    word_to_vec_map -- dictionary mapping words to their GloVe vector representation.\n",
    "    word_to_index -- dictionary mapping from words to their indices in the vocabulary (2MM words)\n",
    "\n",
    "    Returns:\n",
    "    embedding_layer -- pretrained layer Keras instance\n",
    "    \"\"\"\n",
    "    \n",
    "    vocab_len = len(word_to_index) + 1                  # adding 1 to fit Keras embedding (requirement)\n",
    "    emb_dim = word_to_vec_map[\"cucumber\"].shape[0]      # define dimensionality of your GloVe word vectors (= 50)\n",
    "    \n",
    "\n",
    "    # Initialize the embedding matrix as a numpy array of zeros of shape (vocab_len, dimensions of word vectors = emb_dim)\n",
    "    emb_matrix = np.zeros((vocab_len,emb_dim )) #dim is num words by 50\n",
    "    \n",
    "    # Set each row \"index\" of the embedding matrix to be the word vector representation of the \"index\"th word of the vocabulary\n",
    "    j=0\n",
    "    for word, index in word_to_index.items():\n",
    "        if len(word_to_vec_map[word])==50:\n",
    "            j+=1\n",
    "            emb_matrix[index, :] = word_to_vec_map[word]\n",
    "    print(\"number of words with 50 as vector size is: \",j) #only 3 words had vector size less than 50\n",
    "    # Define Keras embedding layer with the correct output/input sizes, make it non-trainable. Use Embedding(...). Make sure to set trainable=False. \n",
    "    embedding_layer = Embedding(input_dim=vocab_len,output_dim=emb_dim,trainable=False)\n",
    "\n",
    "\n",
    "    # Build the embedding layer, it is required before setting the weights of the embedding layer. \n",
    "    embedding_layer.build((None,))\n",
    "    \n",
    "    # Set the weights of the embedding layer to the embedding matrix. Your layer is now pretrained.\n",
    "    embedding_layer.set_weights([emb_matrix])\n",
    "    return embedding_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UzLmJfoBsnUL"
   },
   "outputs": [],
   "source": [
    "#test above code\n",
    "embedding_layer = pretrained_embedding_layer(word_to_vec_map, words_to_index)\n",
    "print(\"weights[0][1][3] =\", embedding_layer.get_weights()[0][1][3]) #weight of 2nd word 3rd position"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ke395SdTsnUN"
   },
   "outputs": [],
   "source": [
    "embedding_layer.get_weights()[0].shape #num words by vector size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Jwh49ezCsnUN"
   },
   "source": [
    "### Step 3: create vanilla RNN model definition - just 1 RNN layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TFRZLKBpsnUO"
   },
   "outputs": [],
   "source": [
    "def Emojify_V2_RNN(input_shape, word_to_vec_map, word_to_index):\n",
    "    \"\"\"\n",
    "    Function creating the Emojify-v2 model's graph.\n",
    "    \n",
    "    Arguments:\n",
    "    input_shape -- shape of the input, usually (max_len,)\n",
    "    word_to_vec_map -- dictionary mapping every word in a vocabulary into its 50-dimensional vector representation\n",
    "    word_to_index -- dictionary mapping from words to their indices in the vocabulary (about 2MM words)\n",
    "\n",
    "    Returns:\n",
    "    model -- a model instance in Keras\n",
    "    \"\"\"\n",
    "\n",
    "    # Define sentence_indices as the input of the graph, it should be of shape input_shape and dtype 'int32' (as it contains indices).\n",
    "    sentence_indices = Input(shape=input_shape,dtype='int32')\n",
    "    # Create the embedding layer pretrained with GloVe Vectors (≈1 line)\n",
    "    embedding_layer = pretrained_embedding_layer(word_to_vec_map, word_to_index)  \n",
    "    # Propagate sentence_indices through your embedding layer, you get back the embeddings\n",
    "    embeddings = embedding_layer(sentence_indices)   \n",
    "    # Propagate the embeddings through a Simple RNN layer with 128-dimensional hidden state\n",
    "    # Be careful, the returned output should be a batch of sequences.\n",
    "    #here, we are using tanh function\n",
    "    X = SimpleRNN(\n",
    "    units=128, \n",
    "    activation='tanh', \n",
    "    use_bias=True,\n",
    "    kernel_initializer='glorot_uniform',\n",
    "    recurrent_initializer='orthogonal',\n",
    "    bias_initializer='zeros', \n",
    "    kernel_regularizer=None,\n",
    "    recurrent_regularizer=None, \n",
    "    bias_regularizer=None, \n",
    "    activity_regularizer=None,\n",
    "    kernel_constraint=None, \n",
    "    recurrent_constraint=None, \n",
    "    bias_constraint=None,\n",
    "    dropout=0.0, \n",
    "    recurrent_dropout=0.0, \n",
    "    return_sequences=False, #true means return the full sequence, i.e. many to many RNN\n",
    "    return_state=False,\n",
    "    go_backwards=False, \n",
    "    stateful=False, \n",
    "    unroll=False\n",
    "    )(embeddings)\n",
    "    # Add dropout with a probability of 0.5\n",
    "    #X = Dropout(0.5)(X)\n",
    "    # Propagate X trough another RNN layer with 128-dimensional hidden state\n",
    "    # Be careful, the returned output should be a single hidden state, not a batch of sequences.\n",
    "#     X = SimpleRNN(units=128, \n",
    "#                   activation='tanh',\n",
    "#                   return_sequences=False)(X) #this is the 2nd layer\n",
    "    # Add dropout with a probability of 0.5\n",
    "    #X = Dropout(0.5)(X)\n",
    "    # Propagate X through a Dense layer with softmax activation to get back a batch of 5-dimensional vectors.\n",
    "    X = Dense(units=2, activation=\"softmax\")(X) #here, we actually could just have 1 unit and have sigmoid, but \n",
    "    #with more categories to predict, we would need more units - one unit for each category after which we would apply \n",
    "    #the softmax activation for each.\n",
    "    \n",
    "    # Create Model instance which converts sentence_indices into X.\n",
    "    model = Model(inputs=sentence_indices, outputs=X)\n",
    "    return model \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "shX8ZdUwsnUP"
   },
   "outputs": [],
   "source": [
    "#below, we start with a single x_i observation, which is a sequence of k words. Since the maximum number of words we can \n",
    "#have is 59 - max number of words, so we use padding - a single observation just has 59 words/tokens. So below, the \n",
    "#Input layer has shape (None, 59), since we do not know how many examples will be processed in a single epoch step - will \n",
    "#depend on the batch size - the batch size hyper parameter also controls the accuracy based on the ability to avoid local \n",
    "#minima, when applicable. \n",
    "#In the embedding layer, each word is given a vector of size 50 (hyper parameter) in this case, so we can have 50 features. \n",
    "#Two words can have similar vectors based on context, but they will not be identical, so the number of parameter learned here\n",
    "#is: total number of words*50 = 1193514*50=59675700, which is very close to the number of parameters in the embedding layer\n",
    "#below - not sure why it is 50 more. So here, keras knows that based on the dictionary we feed in there will be a total \n",
    "#of this many words in the corpus we obtained. Then, the number of pre trained weights has to be 50 per each of these words. \n",
    "\n",
    "#Now, the output shape of the embedding layer is (None, 59, 50) embedding matrix X, which again is None for the batch size, \n",
    "#and then we have 59 words per observation, each word having 50 elements in the embedding vector. This will be the input\n",
    "#into the subsequent RNN layer.\n",
    "\n",
    "#next question is how many parameters are trained in a single RNN layer? So here, this single RNN layer consists of \n",
    "#128 hidden units per time step and it consists of 59 hidden cells, each of which corresponds to a single word in a \n",
    "#single observation. Thus, because the parameters are shared across observations and across time steps, the total number\n",
    "#of parameters to be learned in a single layer for the W_h matrix is: \n",
    "#number of hidden units in the previous time step by (number of hidden units plus number of features in a single time step word) = \n",
    "#128 by (128 + 50) plus a bias term of (128 by 1) = 22,912, which is the number of parameters to be learned in a single\n",
    "#RNN layer below. So essentially, we have to learn the weights for each of the features and also for each of the hidden \n",
    "#unit components of the hidden unit in the prior time step. \n",
    "\n",
    "#After a single RNN layer above, we have as output 128 by 1 hidden unit after the last time step RNN cell of the layer. So \n",
    "#in the layer, it says (None, 128), since again it depends on the batch size. So although the parameters to be learned are\n",
    "#shared across observations and time steps, the number of hidden units actually computed using shared weights (each of which\n",
    "#has 128 components) is equal to the number of observations / input sentences and are different. \n",
    "\n",
    "#Finally, for a single observation, we take the last hidden unit and we feed it into a dense layer. So, (1 by 128) is \n",
    "#fed into a dense layer of 2 units to get the softmax probability of each class. So the final W_yh matrix for which\n",
    "#parameters need to be learned must be of shape 2 by 128 where 2 is the number of classes and 128 is num of hidden units, \n",
    "#so 2 by 128 times 128 by 1 is a resulting 2 by 1 vector, on which we can apply the softmax activation and get the resulting\n",
    "#probabilities. So again, here, we will have 128 by 1 final hidden unit at the last time step different for each observation, \n",
    "#on which we will apply the 2 by 128 W_yh matrix of the same weights to get the final prediction after softmax activation. So, \n",
    "#the final number of parameters to learn is 2*128 + 2*1 for a bias term = 258 total learned parameters - again, here 2 is the \n",
    "#number of classes in the final dependent variable y that need to be predicted. This matches the last layer below. \n",
    "\n",
    "#below, the output shape of the dense layer is (None, 2), since we will have 2 predictions for each observation, and we\n",
    "#do not know how many observations we will need to or want to predict yet. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OIqrTkb7snUS"
   },
   "outputs": [],
   "source": [
    "model = Emojify_V2_RNN((maxlen,), word_to_vec_map, words_to_index)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "s-lQC8SXsnUX"
   },
   "outputs": [],
   "source": [
    "def convert_to_one_hot(Y, C):\n",
    "    Y = np.eye(C)[Y.reshape(-1)]\n",
    "    return Y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OvZObIv6snUX"
   },
   "source": [
    "### Step 4: fit model above on training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wqLfg03ksnUX"
   },
   "outputs": [],
   "source": [
    "#let us now try fitting this simple model: \n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "#we can run the Adam optimizer and track the overall 'accuracy' metric\n",
    "X_train_indices = sentences_to_indices(X_train, words_to_index, maxlen)\n",
    "#convert to one hot encoding\n",
    "Y_train_oh = convert_to_one_hot(Y_train, C = 2) #we have 2 classes\n",
    "model.fit(X_train_indices, Y_train_oh, epochs = 50, batch_size = 32, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vVlE2jZesnUY"
   },
   "source": [
    "### Step 5: Evaluate model performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5kFty9SpsnUY"
   },
   "outputs": [],
   "source": [
    "X_test_indices = sentences_to_indices(X_test, words_to_index, max_len = maxlen)\n",
    "Y_test_oh = convert_to_one_hot(Y_test, C = 2)\n",
    "loss, acc = model.evaluate(X_test_indices, Y_test_oh)\n",
    "print()\n",
    "print(\"Test accuracy = \", acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "y43oRzc4snUZ"
   },
   "outputs": [],
   "source": [
    "loss, acc = model.evaluate(X_train_indices, Y_train_oh)\n",
    "print()\n",
    "print(\"Train accuracy = \", acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "g-1rndmfsnUZ"
   },
   "outputs": [],
   "source": [
    "#note above that the model is over fitting on the test set!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "Copy of BIA682_TwitterDataCleaningPreprocessing - RunVanillaRNN.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
