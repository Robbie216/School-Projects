{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Working with Twitter data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this lecture, you will learn how to preprocess actual twitter data and create word embeddings from a pre-trained source. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import the necessary modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "np.random.seed(0)\n",
    "from keras.models import Model\n",
    "from keras.layers import Dense, Input, Dropout, LSTM, Activation, SimpleRNN, GRU\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.preprocessing import sequence\n",
    "from keras.initializers import glorot_uniform"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Do some initial cleaning using the twitter preprocessing Python package\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Max\\Documents\\TwitterDataModeling\\TwitterData\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "442"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#step 1: bring in the cleaned up twitter data: used preprocessing python package to clean up the initial twitter data\n",
    "%cd C:\\Users\\Max\\Documents\\TwitterDataModeling\\TwitterData\n",
    "import pandas as pd\n",
    "twitter_labeled=pd.read_csv(\"US_AMEX_Tweets_recent2_MV_labels2_05012020_05042020_v2.csv\", encoding=\"utf-8-sig\")\n",
    "#need to save as utf-8 .csv file from excel or other source\n",
    "twitter_labeled2=twitter_labeled.drop_duplicates()\n",
    "dups=pd.DataFrame(twitter_labeled2['text'].value_counts()).rename(columns={'text':'count'})\n",
    "twitter_labeled3=pd.merge(twitter_labeled2, dups, left_on='text', right_index=True, how=\"inner\")\n",
    "twitter_labeled4=twitter_labeled3.drop(axis=1, index=[226, 760, 235, 757, 247, 759, 535, 330, 680, 780])\n",
    "twitter_labeled4.drop('count', axis=1, inplace=True)\n",
    "len(twitter_labeled4) #we have 442 unique tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['text', 'Label'], dtype='object')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "twitter_labeled4.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Since the goal here is to capture negative sentiment, let us convert into 2 classes only: 'negative' and 'non-negative'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(300, 142, 300, 142, 442, 442)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#step 2: only consider 2 classes, negative and non negative, since we want to capture the negatuve sentiment: create\n",
    "#training and test sets\n",
    "twitter_labeled4.loc[:, 'Label_comb']=twitter_labeled4['Label'].map(lambda x: 0 if x==-1 else 1)\n",
    "twitter_labeled4['Label_comb'].value_counts()\n",
    "neg_tweets=twitter_labeled4[twitter_labeled4['Label_comb']==0]\n",
    "non_neg_tweets=twitter_labeled4[twitter_labeled4['Label_comb']==1]\n",
    "X_train=np.array([x for x in non_neg_tweets['text'][:150]]+[x for x in neg_tweets['text'][:150]])\n",
    "Y_train=np.array([x for x in non_neg_tweets['Label_comb'][:150]]+[x for x in neg_tweets['Label_comb'][:150]])\n",
    "X_test=np.array([x for x in non_neg_tweets['text'][150:]]+[x for x in neg_tweets['text'][150:]])\n",
    "Y_test=np.array([x for x in non_neg_tweets['Label_comb'][150:]]+[x for x in neg_tweets['Label_comb'][150:]])\n",
    "X_train_test=np.array([x for x in X_train]+[x for x in X_test])\n",
    "y_train_test=np.array([x for x in Y_train]+[x for x in Y_test])\n",
    "len(X_train), len(X_test), len(Y_train), len(Y_test), len(X_train_test), len(y_train_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import pre-trained glove word embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#step 3: bring in word embedding pre-trained vectors: \n",
    "import numpy as np\n",
    "glove_twitter_file='glove.twitter.27B.50d.txt'\n",
    "def read_glove_vecs(glove_file):\n",
    "    with open(glove_file, 'r', encoding='utf-8-sig') as f:\n",
    "        words = set()\n",
    "        word_to_vec_map = {}\n",
    "        for line in f:\n",
    "            line = line.strip().split()#removes leading and trailing spaces\n",
    "            curr_word = line[0]\n",
    "            words.add(curr_word)\n",
    "            word_to_vec_map[curr_word] = np.array(line[1:], dtype=np.float64)\n",
    "        \n",
    "        i = 1\n",
    "        words_to_index = {}\n",
    "        index_to_words = {}\n",
    "        for w in sorted(words):\n",
    "            words_to_index[w] = i\n",
    "            index_to_words[i] = w\n",
    "            i = i + 1\n",
    "    return words_to_index, index_to_words, word_to_vec_map\n",
    "words_to_index, index_to_words, word_to_vec_map=read_glove_vecs(glove_twitter_file)\n",
    "wordsintwitterglove=[]\n",
    "for value in index_to_words.values(): \n",
    "    wordsintwitterglove.append(value)\n",
    "#almost 2MM words\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1193514"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(wordsintwitterglove)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The word 'cards' in the embedding space is: \n",
      "[ 0.49513   0.0292   -0.41095   0.2833    0.94241  -0.37731   0.84692\n",
      " -0.51128   0.45714  -0.58924   0.94307   0.90303  -3.4979    0.29424\n",
      " -0.26355   0.42858  -0.38724  -0.47719   0.044124  0.49529  -1.1181\n",
      " -0.48781   0.60082  -0.64361  -0.71212   0.36798  -0.059819 -0.58809\n",
      "  0.67646  -0.75717   0.4728   -0.23525  -0.29401  -0.13993   0.69861\n",
      " -0.29542  -0.013882  0.065944 -0.38697   0.23558   0.50186   0.09126\n",
      "  0.4026   -0.39129   0.73219  -0.52371  -0.048465 -1.2898   -0.022145\n",
      "  0.42831 ]\n"
     ]
    }
   ],
   "source": [
    "print(\"The word 'cards' in the embedding space is: \")\n",
    "print(word_to_vec_map['cards'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Typical steps to clean the data for sentiment analysis: \n",
    "* Remove hash tags and other distracting symbols\n",
    "* Remove numbers\n",
    "* Convert to lower case\n",
    "* Remove unnecessary punctuation\n",
    "* Remove stop words: unnecessary/generic words\n",
    "* Stem sentences: only retain the main roots\n",
    "* Tokenize sentences: create a list of words from the final cleaned string above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original sentence is: \n",
      " \n",
      "I used to work for American Express and spent some time in NY. It was on her bucket list to visit, so I took her there a year and a half ago. We love living around no one, but NYC is a fun place to visit! Well probably not so much at the moment, but we’ll get through this!\n",
      " \n",
      "Tokenized sentence is now: \n",
      " \n",
      "['i', 'used', 'to', 'work', 'for', 'american', 'express', 'and', 'spent', 'some', 'time', 'in', 'ny', 'it', 'was', 'on', 'her', 'bucket', 'list', 'to', 'visit', 'so', 'i', 'took', 'her', 'there', 'a', 'year', 'and', 'a', 'half', 'ago', 'we', 'love', 'living', 'around', 'no', 'one', 'but', 'nyc', 'is', 'a', 'fun', 'place', 'to', 'visit', 'well', 'probably', 'not', 'so', 'much', 'at', 'the', 'moment', 'but', 'well', 'get', 'through', 'this']\n"
     ]
    }
   ],
   "source": [
    "#step 4: function to clean this twitter data some more to create words that can be mapped to the embedding matrix: \n",
    "#function to take in a sentence and return a list of words, which are more ready to be vectorized by the embedding matrix: \n",
    "def clean_sentence(sentence): \n",
    "    import re\n",
    "    import num2word\n",
    "    from num2words import num2words\n",
    "    punc_to_remove='’!\"#%&\\'()*+,/:;<=>?@[\\\\]^_`{|}~' #exclude dollar sign and dot as we want to first convert dollar amounts\n",
    "    #and decimals to word numerics, and then we can remove these punctuation signs: \n",
    "    words=re.split('\\s+',sentence.lower())\n",
    "    table = str.maketrans('', '', punc_to_remove) #remove punctuation as it could be at the end of the word\n",
    "    words = [w.translate(table) for w in words]\n",
    "    words=[x for x in words if x!='']\n",
    "    sentence=\" \".join(words)\n",
    "    #first replace only the actual numbers with words: \n",
    "    th_pattern='\\d+th'\n",
    "    th_numbers_to_fix=re.findall(th_pattern, sentence) #list of th numbers to convert to numerics\n",
    "    dollar_pattern='\\$\\.?0?\\d+\\.?\\d*'\n",
    "    dollars_to_fix=re.findall(dollar_pattern, sentence)\n",
    "    words=re.split('\\s+',sentence.lower())\n",
    "    remove_numerics=[]\n",
    "    for word in words: \n",
    "        try:\n",
    "            if(word in th_numbers_to_fix):            \n",
    "                remove_numerics.append(num2words(int(float(word.replace('th',''))), to='ordinal'))\n",
    "            elif (word in dollars_to_fix): \n",
    "                number=int(float(word.replace('$','')))\n",
    "                if(number==1): \n",
    "                    remove_numerics.append(num2words(int(float(word.replace('$','')))))\n",
    "                    remove_numerics.append('dollar')\n",
    "                else: \n",
    "                    remove_numerics.append(num2words(int(float(word.replace('$','')))))\n",
    "                    remove_numerics.append('dollars')  \n",
    "            else:\n",
    "                number=int(float(word)) #convert decimal or integer string into an integer\n",
    "                remove_numerics.append(num2word.word(number))\n",
    "        except: \n",
    "            remove_numerics.append(word)\n",
    "    punc_to_remove='$.' #exclude dollar sign\n",
    "    sentence=\" \".join(remove_numerics)\n",
    "    words=re.split('\\s+',sentence.lower())\n",
    "    table = str.maketrans('', '', punc_to_remove) #remove punctuation as it could be at the end of the word\n",
    "    words = [w.translate(table) for w in words]\n",
    "    words=[x for x in words if x not in ('', '-')]\n",
    "    return words\n",
    "sentence=\"I used to work for American Express and spent some time in NY. It was on her bucket list to visit, so I took her there a year and a half ago. We love living around no one, but NYC is a fun place to visit! Well probably not so much at the moment, but we’ll get through this!\"\n",
    "words=clean_sentence(sentence)\n",
    "print(\"Original sentence is: \")\n",
    "print(\" \")\n",
    "print(sentence)\n",
    "print(\" \")\n",
    "print(\"Tokenized sentence is now: \")\n",
    "print(\" \")\n",
    "print(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\Max\\\\Documents\\\\TwitterDataModeling\\\\TwitterData'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now, let us look at our training and test sets: \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Question of the Day - Is it worth the effort to get the Hilton Ascend credit card that can be linked to Wyndham Diamond and Caesars Diamond status rewards cards?',\n",
       "       'Today’s Digital Transactions News: T&E Plunge Hammers AmEx; PayFac Volume To Hit $4 Trillion by 2025; Plus Weekly Recap'],\n",
       "      dtype='<U280')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train[0:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['rumor has it amex might be planning additional benefits to their platinum card... meanwhile the sapphire took a downfall with their recent updates (imo anyway....the benefits are useless to me making the price increase not worth it)',\n",
       "       '[Targeted] AmEx Offer: , Spend $100+ & Receive $30 Statement Credit + $50 Off $150+'],\n",
       "      dtype='<U280')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test[0:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: find the maximum number of words across all sentences. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentences_to_indices(X, word_to_index, max_len):\n",
    "    \"\"\"\n",
    "    Converts an array of sentences (strings) into an array of indices corresponding to words in the sentences.\n",
    "    The output shape should be such that it can be given to `Embedding()` \n",
    "    \n",
    "    Arguments:\n",
    "    X -- array of sentences (strings), of shape (m, 1)\n",
    "    word_to_index -- a dictionary containing the each word mapped to its index\n",
    "    max_len -- maximum number of words in a sentence. You can assume every sentence in X is no longer than this. \n",
    "    \n",
    "    Returns:\n",
    "    X_indices -- array of indices corresponding to words in the sentences from X, of shape (m, max_len)\n",
    "    \"\"\"\n",
    "    \n",
    "    m = X.shape[0]                                   # number of training examples\n",
    "    \n",
    "    # Initialize X_indices as a numpy matrix of zeros and the correct shape (≈ 1 line)\n",
    "    X_indices = np.zeros((m,max_len))\n",
    "    \n",
    "    for i in range(m):                               # loop over training examples\n",
    "        \n",
    "        # Convert the ith training sentence in lower case and split is into words. You should get a list of words.\n",
    "        sentence_words=clean_sentence(X[i])\n",
    "        # Initialize j to 0\n",
    "        j = 0\n",
    "        \n",
    "        # Loop over the words of sentence_words\n",
    "        for w in sentence_words:\n",
    "            # Set the (i,j)th entry of X_indices to the index of the correct word.\n",
    "            if (w in wordsintwitterglove): \n",
    "                X_indices[i, j] = word_to_index[w]\n",
    "                # Increment j to j + 1\n",
    "                j = j+1\n",
    "            \n",
    "\n",
    "    \n",
    "    return X_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X1 = ['Question of the Day - Is it worth the effort to get the Hilton Ascend credit card that can be linked to Wyndham Diamond and Caesars Diamond status rewards cards?'\n",
      " 'Today’s Digital Transactions News: T&E Plunge Hammers AmEx; PayFac Volume To Hit $4 Trillion by 2025; Plus Weekly Recap']\n",
      "X1_indices = [[503357. 446383. 601627. 138215. 283380. 284816. 657072. 601627. 176556.\n",
      "  607687. 227866. 601627. 258111.  37963. 127558.  96730. 601405.  94350.\n",
      "   59105. 342820. 607687. 658588. 151100.  26338.  91709. 151100. 573728.\n",
      "  519712.  96819.      0.      0.      0.      0.      0.      0.      0.\n",
      "       0.      0.      0.      0.      0.      0.      0.      0.      0.\n",
      "       0.      0.      0.      0.      0.      0.      0.      0.      0.\n",
      "       0.      0.      0.      0.      0.]\n",
      " [608104. 153374. 612987. 406728. 593762. 483492. 248627.  24377. 644128.\n",
      "  607687. 259017. 213327. 163389. 615083.  89093. 621963. 603283. 620819.\n",
      "  208201. 483525. 650553. 510109.      0.      0.      0.      0.      0.\n",
      "       0.      0.      0.      0.      0.      0.      0.      0.      0.\n",
      "       0.      0.      0.      0.      0.      0.      0.      0.      0.\n",
      "       0.      0.      0.      0.      0.      0.      0.      0.      0.\n",
      "       0.      0.      0.      0.      0.]]\n"
     ]
    }
   ],
   "source": [
    "#testing the above: \n",
    "X1 =  X_train[0:2]\n",
    "maxlen=max(max([len(x.split()) for x in X_train]), max([len(x.split()) for x in X_test]))\n",
    "X1_indices = sentences_to_indices(X1,words_to_index, max_len = maxlen)\n",
    "print(\"X1 =\", X1)\n",
    "print(\"X1_indices =\", X1_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#notice above that the words need to be padded to make sure each input sentence takes up the same amount of static\n",
    "#memory when being fed into a Recurrent Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Define an embedding layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pretrained_embedding_layer(word_to_vec_map, word_to_index):\n",
    "    \"\"\"\n",
    "    Creates a Keras Embedding() layer and loads in pre-trained GloVe 50-dimensional vectors.\n",
    "    \n",
    "    Arguments:\n",
    "    word_to_vec_map -- dictionary mapping words to their GloVe vector representation.\n",
    "    word_to_index -- dictionary mapping from words to their indices in the vocabulary (2MM words)\n",
    "\n",
    "    Returns:\n",
    "    embedding_layer -- pretrained layer Keras instance\n",
    "    \"\"\"\n",
    "    \n",
    "    vocab_len = len(word_to_index) + 1                  # adding 1 to fit Keras embedding (requirement)\n",
    "    emb_dim = word_to_vec_map[\"cucumber\"].shape[0]      # define dimensionality of your GloVe word vectors (= 50)\n",
    "    \n",
    "\n",
    "    # Initialize the embedding matrix as a numpy array of zeros of shape (vocab_len, dimensions of word vectors = emb_dim)\n",
    "    emb_matrix = np.zeros((vocab_len,emb_dim )) #dim is num words by 50\n",
    "    \n",
    "    # Set each row \"index\" of the embedding matrix to be the word vector representation of the \"index\"th word of the vocabulary\n",
    "    j=0\n",
    "    for word, index in word_to_index.items():\n",
    "        if len(word_to_vec_map[word])==50:\n",
    "            j+=1\n",
    "            emb_matrix[index, :] = word_to_vec_map[word]\n",
    "    print(\"number of words with 50 as vector size is: \",j) #only 3 words had vector size less than 50\n",
    "    # Define Keras embedding layer with the correct output/input sizes, make it non-trainable. Use Embedding(...). Make sure to set trainable=False. \n",
    "    embedding_layer = Embedding(input_dim=vocab_len,output_dim=emb_dim,trainable=False)\n",
    "\n",
    "\n",
    "    # Build the embedding layer, it is required before setting the weights of the embedding layer. \n",
    "    embedding_layer.build((None,))\n",
    "    \n",
    "    # Set the weights of the embedding layer to the embedding matrix. Your layer is now pretrained.\n",
    "    embedding_layer.set_weights([emb_matrix])\n",
    "    return embedding_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of words with 50 as vector size is:  1193513\n",
      "weights[0][1][3] = -0.80743\n"
     ]
    }
   ],
   "source": [
    "#test above code\n",
    "embedding_layer = pretrained_embedding_layer(word_to_vec_map, words_to_index)\n",
    "print(\"weights[0][1][3] =\", embedding_layer.get_weights()[0][1][3]) #weight of 2nd word 3rd position"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1193515, 50)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_layer.get_weights()[0].shape #num words by vector size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: create LSTM model definition - just 1 LSTM layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Emojify_V2_LSTM_OneLayer(input_shape, word_to_vec_map, word_to_index):\n",
    "    \"\"\"\n",
    "    Function creating the Emojify-v2 model's graph.\n",
    "    \n",
    "    Arguments:\n",
    "    input_shape -- shape of the input, usually (max_len,)\n",
    "    word_to_vec_map -- dictionary mapping every word in a vocabulary into its 50-dimensional vector representation\n",
    "    word_to_index -- dictionary mapping from words to their indices in the vocabulary (about 2MM words)\n",
    "\n",
    "    Returns:\n",
    "    model -- a model instance in Keras\n",
    "    \"\"\"\n",
    "    \n",
    "    ### START CODE HERE ###\n",
    "    # Define sentence_indices as the input of the graph, it should be of shape input_shape and dtype 'int32' (as it contains indices).\n",
    "    sentence_indices = Input(shape=input_shape,dtype='int32')\n",
    "    # Create the embedding layer pretrained with GloVe Vectors (≈1 line)\n",
    "    embedding_layer = pretrained_embedding_layer(word_to_vec_map, word_to_index)  \n",
    "    # Propagate sentence_indices through your embedding layer, you get back the embeddings\n",
    "    embeddings = embedding_layer(sentence_indices)   \n",
    "    # Propagate the embeddings through a Simple RNN layer with 128-dimensional hidden state\n",
    "    # Be careful, the returned output should be a batch of sequences.\n",
    "    #here, we are using tanh function\n",
    "    X = LSTM(\n",
    "    units=128,\n",
    "    activation=\"tanh\", #this is for the hidden unit update - both c tilda and final activation unit\n",
    "    recurrent_activation=\"sigmoid\", #this is for Gamma update and relevance gates computation\n",
    "    use_bias=True,\n",
    "    kernel_initializer=\"glorot_uniform\",\n",
    "    recurrent_initializer=\"orthogonal\",\n",
    "    bias_initializer=\"zeros\",\n",
    "    kernel_regularizer=None,\n",
    "    recurrent_regularizer=None,\n",
    "    bias_regularizer=None,\n",
    "    activity_regularizer=None,\n",
    "    kernel_constraint=None,\n",
    "    recurrent_constraint=None,\n",
    "    bias_constraint=None,\n",
    "    dropout=0.0,\n",
    "    recurrent_dropout=0.0,\n",
    "    return_sequences=False, #here just want to output the last time unit prediction\n",
    "    return_state=False,\n",
    "    go_backwards=False,\n",
    "    stateful=False,\n",
    "    unroll=False\n",
    "    #time_major=False,\n",
    "    #reset_after=False #if true, then there will be more bias terms. \n",
    "    )(embeddings)\n",
    "    # Add dropout with a probability of 0.5\n",
    "    #X = Dropout(0.5)(X)\n",
    "    # Add dropout with a probability of 0.5\n",
    "    #X = Dropout(0.5)(X)\n",
    "    # Propagate X through a Dense layer with softmax activation to get back a batch of 5-dimensional vectors.\n",
    "    X = Dense(units=2, activation=\"softmax\")(X) #here, we actually could just have 1 unit and have sigmoid, but \n",
    "    #with more categories to predict, we would need more units - one unit for each category after which we would apply \n",
    "    #the softmax activation for each.\n",
    "    # Create Model instance which converts sentence_indices into X.\n",
    "    model = Model(inputs=sentence_indices, outputs=X)\n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#below, for the LSTM layer, we need to learn 3 gates + the main hidden unit tilda weight matrix, so we need to learn in \n",
    "#total: \n",
    "#128 by (128 + 50) + (128 by 1) and then this 4 times, which equals to: \n",
    "#4*(128*(128+50)+128)=91,648\n",
    "#the last layer needs to take in the 128 final t output activation units and input them into a 2 unit dense layer, since \n",
    "#we have 2 classes. This means, we need to learn 2 by 128 weights plus bias, so number of parameters to be learned here is: \n",
    "#2*128+2=258 parameters still"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of words with 50 as vector size is:  1193513\n",
      "Model: \"model_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_2 (InputLayer)         (None, 59)                0         \n",
      "_________________________________________________________________\n",
      "embedding_3 (Embedding)      (None, 59, 50)            59675750  \n",
      "_________________________________________________________________\n",
      "lstm_2 (LSTM)                (None, 128)               91648     \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 2)                 258       \n",
      "=================================================================\n",
      "Total params: 59,767,656\n",
      "Trainable params: 91,906\n",
      "Non-trainable params: 59,675,750\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Emojify_V2_LSTM_OneLayer((maxlen,), word_to_vec_map, words_to_index)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_one_hot(Y, C):\n",
    "    Y = np.eye(C)[Y.reshape(-1)]\n",
    "    return Y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: fit model above on training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Max\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:422: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      "\n",
      "Epoch 1/50\n",
      "300/300 [==============================] - 1s 4ms/step - loss: 0.6906 - accuracy: 0.5867\n",
      "Epoch 2/50\n",
      "300/300 [==============================] - 1s 2ms/step - loss: 0.6693 - accuracy: 0.6267\n",
      "Epoch 3/50\n",
      "300/300 [==============================] - 1s 2ms/step - loss: 0.6622 - accuracy: 0.6433\n",
      "Epoch 4/50\n",
      "300/300 [==============================] - 1s 3ms/step - loss: 0.5706 - accuracy: 0.7300\n",
      "Epoch 5/50\n",
      "300/300 [==============================] - 1s 3ms/step - loss: 0.5313 - accuracy: 0.7767\n",
      "Epoch 6/50\n",
      "300/300 [==============================] - 1s 3ms/step - loss: 0.5175 - accuracy: 0.7667\n",
      "Epoch 7/50\n",
      "300/300 [==============================] - 1s 3ms/step - loss: 0.4864 - accuracy: 0.8100\n",
      "Epoch 8/50\n",
      "300/300 [==============================] - 1s 2ms/step - loss: 0.4760 - accuracy: 0.8033\n",
      "Epoch 9/50\n",
      "300/300 [==============================] - 1s 2ms/step - loss: 0.4370 - accuracy: 0.8367\n",
      "Epoch 10/50\n",
      "300/300 [==============================] - 1s 2ms/step - loss: 0.5398 - accuracy: 0.7633\n",
      "Epoch 11/50\n",
      "300/300 [==============================] - 1s 2ms/step - loss: 0.4901 - accuracy: 0.8000\n",
      "Epoch 12/50\n",
      "300/300 [==============================] - 1s 2ms/step - loss: 0.4513 - accuracy: 0.8033\n",
      "Epoch 13/50\n",
      "300/300 [==============================] - 1s 2ms/step - loss: 0.3947 - accuracy: 0.8367\n",
      "Epoch 14/50\n",
      "300/300 [==============================] - 1s 2ms/step - loss: 0.3637 - accuracy: 0.8667\n",
      "Epoch 15/50\n",
      "300/300 [==============================] - 1s 2ms/step - loss: 0.3790 - accuracy: 0.8467\n",
      "Epoch 16/50\n",
      "300/300 [==============================] - 1s 2ms/step - loss: 0.3526 - accuracy: 0.8733\n",
      "Epoch 17/50\n",
      "300/300 [==============================] - 1s 2ms/step - loss: 0.3238 - accuracy: 0.8767\n",
      "Epoch 18/50\n",
      "300/300 [==============================] - 1s 2ms/step - loss: 0.3352 - accuracy: 0.8700\n",
      "Epoch 19/50\n",
      "300/300 [==============================] - 1s 2ms/step - loss: 0.3237 - accuracy: 0.8733\n",
      "Epoch 20/50\n",
      "300/300 [==============================] - 1s 2ms/step - loss: 0.3246 - accuracy: 0.8900\n",
      "Epoch 21/50\n",
      "300/300 [==============================] - 1s 2ms/step - loss: 0.3555 - accuracy: 0.8667\n",
      "Epoch 22/50\n",
      "300/300 [==============================] - 1s 2ms/step - loss: 0.3107 - accuracy: 0.8867\n",
      "Epoch 23/50\n",
      "300/300 [==============================] - 1s 2ms/step - loss: 0.2953 - accuracy: 0.8967\n",
      "Epoch 24/50\n",
      "300/300 [==============================] - 1s 2ms/step - loss: 0.2630 - accuracy: 0.9067\n",
      "Epoch 25/50\n",
      "300/300 [==============================] - 1s 2ms/step - loss: 0.2285 - accuracy: 0.9133\n",
      "Epoch 26/50\n",
      "300/300 [==============================] - 1s 2ms/step - loss: 0.2347 - accuracy: 0.9233\n",
      "Epoch 27/50\n",
      "300/300 [==============================] - 1s 2ms/step - loss: 0.2124 - accuracy: 0.9333\n",
      "Epoch 28/50\n",
      "300/300 [==============================] - 1s 3ms/step - loss: 0.2066 - accuracy: 0.9300\n",
      "Epoch 29/50\n",
      "300/300 [==============================] - 1s 2ms/step - loss: 0.2301 - accuracy: 0.9200\n",
      "Epoch 30/50\n",
      "300/300 [==============================] - 1s 3ms/step - loss: 0.2769 - accuracy: 0.9000\n",
      "Epoch 31/50\n",
      "300/300 [==============================] - 1s 2ms/step - loss: 0.2241 - accuracy: 0.9367\n",
      "Epoch 32/50\n",
      "300/300 [==============================] - 1s 2ms/step - loss: 0.2682 - accuracy: 0.9033\n",
      "Epoch 33/50\n",
      "300/300 [==============================] - 1s 2ms/step - loss: 0.3115 - accuracy: 0.8900\n",
      "Epoch 34/50\n",
      "300/300 [==============================] - 1s 2ms/step - loss: 0.2688 - accuracy: 0.9267\n",
      "Epoch 35/50\n",
      "300/300 [==============================] - 1s 2ms/step - loss: 0.2013 - accuracy: 0.9300\n",
      "Epoch 36/50\n",
      "300/300 [==============================] - 1s 2ms/step - loss: 0.1796 - accuracy: 0.9500\n",
      "Epoch 37/50\n",
      "300/300 [==============================] - 1s 2ms/step - loss: 0.1501 - accuracy: 0.9600\n",
      "Epoch 38/50\n",
      "300/300 [==============================] - 1s 3ms/step - loss: 0.1124 - accuracy: 0.9733\n",
      "Epoch 39/50\n",
      "300/300 [==============================] - 1s 3ms/step - loss: 0.1406 - accuracy: 0.9633\n",
      "Epoch 40/50\n",
      "300/300 [==============================] - 1s 2ms/step - loss: 0.1624 - accuracy: 0.9500\n",
      "Epoch 41/50\n",
      "300/300 [==============================] - 1s 2ms/step - loss: 0.1254 - accuracy: 0.9633\n",
      "Epoch 42/50\n",
      "300/300 [==============================] - 1s 2ms/step - loss: 0.1409 - accuracy: 0.9633\n",
      "Epoch 43/50\n",
      "300/300 [==============================] - 1s 2ms/step - loss: 0.1034 - accuracy: 0.9767\n",
      "Epoch 44/50\n",
      "300/300 [==============================] - 1s 2ms/step - loss: 0.1130 - accuracy: 0.9667\n",
      "Epoch 45/50\n",
      "300/300 [==============================] - 1s 3ms/step - loss: 0.2114 - accuracy: 0.9333\n",
      "Epoch 46/50\n",
      "300/300 [==============================] - 1s 2ms/step - loss: 0.1845 - accuracy: 0.9467\n",
      "Epoch 47/50\n",
      "300/300 [==============================] - 1s 2ms/step - loss: 0.1240 - accuracy: 0.9600\n",
      "Epoch 48/50\n",
      "300/300 [==============================] - 1s 3ms/step - loss: 0.1792 - accuracy: 0.9333\n",
      "Epoch 49/50\n",
      "300/300 [==============================] - 1s 2ms/step - loss: 0.1917 - accuracy: 0.9333\n",
      "Epoch 50/50\n",
      "300/300 [==============================] - 1s 2ms/step - loss: 0.2057 - accuracy: 0.9400\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x17b151c7d68>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#let us now try fitting this simple model: \n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "#we can run the Adam optimizer and track the overall 'accuracy' metric\n",
    "X_train_indices = sentences_to_indices(X_train, words_to_index, maxlen)\n",
    "#convert to one hot encoding\n",
    "Y_train_oh = convert_to_one_hot(Y_train, C = 2) #we have 2 classes\n",
    "model.fit(X_train_indices, Y_train_oh, epochs = 50, batch_size = 32, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5: Evaluate model performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "142/142 [==============================] - 0s 1ms/step\n",
      "\n",
      "Test accuracy =  0.6971830725669861\n"
     ]
    }
   ],
   "source": [
    "X_test_indices = sentences_to_indices(X_test, words_to_index, max_len = maxlen)\n",
    "Y_test_oh = convert_to_one_hot(Y_test, C = 2)\n",
    "loss, acc = model.evaluate(X_test_indices, Y_test_oh)\n",
    "print()\n",
    "print(\"Test accuracy = \", acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "300/300 [==============================] - 0s 697us/step\n",
      "\n",
      "Train accuracy =  0.9666666388511658\n"
     ]
    }
   ],
   "source": [
    "loss, acc = model.evaluate(X_train_indices, Y_train_oh)\n",
    "print()\n",
    "print(\"Train accuracy = \", acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#note above that the model is over fitting on the test set more than the GRU slightly but much less than the RNN!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
